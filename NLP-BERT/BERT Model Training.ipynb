{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a85d1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-06-11 16:48:32--  https://github.com/Ankit152/IMDB-sentiment-analysis/raw/master/IMDB-Dataset.csv\n",
      "Resolving github.com (github.com)... 52.192.72.89\n",
      "Connecting to github.com (github.com)|52.192.72.89|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv [following]\n",
      "--2022-06-11 16:48:32--  https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 66212309 (63M) [text/plain]\n",
      "Saving to: ‘IMDB-Dataset.csv’\n",
      "\n",
      "IMDB-Dataset.csv    100%[===================>]  63.14M   197MB/s    in 0.3s    \n",
      "\n",
      "2022-06-11 16:48:36 (197 MB/s) - ‘IMDB-Dataset.csv’ saved [66212309/66212309]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/Ankit152/IMDB-sentiment-analysis/raw/master/IMDB-Dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65b66c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "imdb_df = pd.read_csv(\"IMDB-Dataset.csv\")\n",
    "reviews = imdb_df.review.to_string(index=None)\n",
    "with open(\"corpus.txt\", \"w\") as f:\n",
    "      f.writelines(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83646b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.19.4-py3-none-any.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 34.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (0.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 56.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 8.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.6.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n",
      "\u001b[K     |████████████████████████████████| 764 kB 60.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.5.30)\n",
      "Installing collected packages: tokenizers, regex, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.7.0 regex-2022.6.2 tokenizers-0.12.1 transformers-4.19.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91196830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer()\n",
    "bert_wordpiece_tokenizer.train(\"corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6807bb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'noise': 8923,\n",
       " '##bell': 5915,\n",
       " 'cheers': 15459,\n",
       " 'extras': 17485,\n",
       " 'hopeless': 12770,\n",
       " ',': 16,\n",
       " 'leg': 1800,\n",
       " 'trendy': 17337,\n",
       " '##anist': 14460,\n",
       " 'electric': 13321,\n",
       " 'spectac': 3530,\n",
       " '##ional': 1921,\n",
       " 'nat': 2084,\n",
       " 'hobgob': 9077,\n",
       " '##antic': 8001,\n",
       " 'tights': 10959,\n",
       " '##prise': 5284,\n",
       " 'duch': 5643,\n",
       " 'mccarthy': 13713,\n",
       " 'georget': 17652,\n",
       " 'swed': 2764,\n",
       " '##aaa': 12923,\n",
       " 'lucky': 2252,\n",
       " '##gerald': 12707,\n",
       " 'chun': 11882,\n",
       " '##endium': 15198,\n",
       " 'bearing': 16782,\n",
       " 'updating': 9508,\n",
       " 'hair': 3913,\n",
       " 'min': 573,\n",
       " 'mockument': 6510,\n",
       " 'satur': 2057,\n",
       " 'haneke': 9694,\n",
       " 'frank': 1537,\n",
       " 'ludic': 5420,\n",
       " 'vers': 750,\n",
       " 'ev': 866,\n",
       " '##off': 3044,\n",
       " '##enth': 11807,\n",
       " '##lland': 11805,\n",
       " '##imi': 14703,\n",
       " 'stay': 2271,\n",
       " 'diner': 16656,\n",
       " 'oklahoma': 17937,\n",
       " 'instantly': 10698,\n",
       " 'ketchup': 18090,\n",
       " 'videostore': 15527,\n",
       " '##pa': 5810,\n",
       " 'chave': 14758,\n",
       " '##self': 865,\n",
       " 'corrupt': 9186,\n",
       " 'grind': 10333,\n",
       " 'peoples': 10297,\n",
       " 'unique': 2011,\n",
       " 'obel': 12414,\n",
       " 'jung': 4652,\n",
       " 'had': 315,\n",
       " 'kristoff': 9472,\n",
       " 'distinguished': 17791,\n",
       " 'fac': 6281,\n",
       " 'continued': 11007,\n",
       " 'thackeray': 18148,\n",
       " 'harlin': 15657,\n",
       " 'writi': 12390,\n",
       " 'tight': 4018,\n",
       " 'opened': 4825,\n",
       " 'see': 292,\n",
       " 'information': 7008,\n",
       " 'obser': 12906,\n",
       " 'infidelity': 18269,\n",
       " 'count': 1630,\n",
       " 'wing': 5792,\n",
       " 'uninspiring': 17308,\n",
       " 'examine': 15792,\n",
       " 'enthralling': 13716,\n",
       " '##pt': 445,\n",
       " 'inno': 3204,\n",
       " '##ry': 500,\n",
       " 'critically': 9158,\n",
       " 'alice': 3923,\n",
       " '##ounded': 5555,\n",
       " 'kersh': 14014,\n",
       " '##ji': 5803,\n",
       " 'thug': 14393,\n",
       " '##res': 1584,\n",
       " 'wandered': 18136,\n",
       " 'although': 699,\n",
       " 'rebecca': 13663,\n",
       " '##nett': 11558,\n",
       " '##infeld': 11682,\n",
       " 'kok': 8678,\n",
       " 'submarine': 7664,\n",
       " 'ungar': 12047,\n",
       " 'readers': 6299,\n",
       " 'acknowled': 9625,\n",
       " '##ovan': 9322,\n",
       " 'impulse': 10468,\n",
       " 'mockumentary': 8551,\n",
       " '.': 18,\n",
       " '##inar': 14410,\n",
       " 'pintilie': 11217,\n",
       " 'mandolin': 8580,\n",
       " 'distorted': 16009,\n",
       " 'andrews': 5411,\n",
       " 'mal': 2523,\n",
       " 'wy': 7853,\n",
       " 'dangerous': 6492,\n",
       " 'poirot': 12037,\n",
       " '##amenco': 14664,\n",
       " 'endi': 15528,\n",
       " '##illa': 4346,\n",
       " 'line': 1313,\n",
       " 'joe': 2004,\n",
       " 'situations': 17971,\n",
       " 'decade': 4561,\n",
       " 'atroc': 5398,\n",
       " 'quant': 8056,\n",
       " '##ene': 1778,\n",
       " '##rupt': 8416,\n",
       " 'touching': 2431,\n",
       " 'jail': 13998,\n",
       " '33': 7785,\n",
       " '##ell': 968,\n",
       " 'cla': 5258,\n",
       " 'ready': 4983,\n",
       " 'sooooo': 17278,\n",
       " 'assume': 5646,\n",
       " 'gleas': 13949,\n",
       " 'shout': 14678,\n",
       " 'sai': 8867,\n",
       " '##amboat': 10056,\n",
       " 'mother': 2088,\n",
       " 'nimoy': 17856,\n",
       " 'informing': 16779,\n",
       " 'agreed': 7693,\n",
       " 'empt': 6362,\n",
       " 'excuse': 2463,\n",
       " 'seriously': 1581,\n",
       " 'conduct': 13054,\n",
       " 'ignore': 6486,\n",
       " '##´t': 8791,\n",
       " 'other': 587,\n",
       " 'susie': 12944,\n",
       " 'confuses': 15926,\n",
       " 'co': 538,\n",
       " 'command': 5619,\n",
       " 'martial': 3249,\n",
       " 'dandy': 18061,\n",
       " 'precis': 12026,\n",
       " '##etically': 14667,\n",
       " 'piti': 8699,\n",
       " 'cunningham': 17824,\n",
       " 'susp': 1946,\n",
       " 'witherspoon': 11236,\n",
       " 'todays': 17531,\n",
       " 'blandings': 17159,\n",
       " '##mons': 7894,\n",
       " 'countryside': 16592,\n",
       " 'touch': 2818,\n",
       " 'clifford': 17242,\n",
       " 'rugrats': 18250,\n",
       " 'tension': 7177,\n",
       " '##cier': 10349,\n",
       " 'coward': 7581,\n",
       " 'pied': 14079,\n",
       " 'studen': 12492,\n",
       " '##arre': 2531,\n",
       " 'awed': 15153,\n",
       " 'countr': 16154,\n",
       " 'dirk': 12562,\n",
       " '##oven': 14278,\n",
       " '##obs': 9909,\n",
       " 'ishimoto': 17151,\n",
       " '##table': 4318,\n",
       " 'artyfart': 17633,\n",
       " 'danes': 10575,\n",
       " 'destination': 17239,\n",
       " 'rome': 4147,\n",
       " 'important': 2480,\n",
       " '##ulations': 11880,\n",
       " 'nurse': 4128,\n",
       " 'mister': 10464,\n",
       " 'losing': 6946,\n",
       " 'bachan': 13075,\n",
       " 'golde': 16079,\n",
       " 'classified': 10398,\n",
       " 'seasoning': 16140,\n",
       " 'picture': 1379,\n",
       " 'nab': 14044,\n",
       " '##iac': 3151,\n",
       " 'nights': 3231,\n",
       " 'favorable': 12304,\n",
       " 'abd': 10076,\n",
       " 'crush': 7372,\n",
       " 'oliv': 5093,\n",
       " 'hyp': 5642,\n",
       " 'wolfe': 13531,\n",
       " 'fact': 1025,\n",
       " '##tz': 6204,\n",
       " 'entertainm': 12433,\n",
       " 'interaction': 17783,\n",
       " 'bathebo': 18056,\n",
       " 'conjure': 17743,\n",
       " '##em': 245,\n",
       " '##ains': 2751,\n",
       " 'sara': 7173,\n",
       " '##og': 577,\n",
       " '##caster': 11566,\n",
       " 'creators': 4821,\n",
       " 'triangle': 9647,\n",
       " 'journ': 16439,\n",
       " 'contra': 15756,\n",
       " 'dish': 10280,\n",
       " 'unherald': 15041,\n",
       " '##astern': 6356,\n",
       " '##8': 124,\n",
       " 'kas': 9791,\n",
       " 'cockney': 17346,\n",
       " 'sporadically': 17905,\n",
       " 'wondrous': 17943,\n",
       " 'daytime': 15698,\n",
       " 'kettle': 13757,\n",
       " '##outh': 14728,\n",
       " 'cynthia': 13181,\n",
       " 'raines': 10772,\n",
       " 'appearances': 16146,\n",
       " 'lifet': 3853,\n",
       " 'jen': 6162,\n",
       " 'rarel': 16384,\n",
       " 'sandy': 12812,\n",
       " 'dekalog': 17912,\n",
       " 'user': 3594,\n",
       " 'eden': 15496,\n",
       " 'reas': 7238,\n",
       " 'th': 133,\n",
       " 'hating': 11415,\n",
       " '##ples': 10310,\n",
       " 'inan': 14521,\n",
       " '##izes': 5917,\n",
       " '##nedy': 8201,\n",
       " 'canine': 11955,\n",
       " '##arr': 1718,\n",
       " '##ades': 4947,\n",
       " 'immers': 16724,\n",
       " 'luis': 5943,\n",
       " 'maniratnam': 14905,\n",
       " 'closing': 8408,\n",
       " 'sugi': 14806,\n",
       " '##omat': 11749,\n",
       " '##ulo': 14745,\n",
       " 'tess': 9844,\n",
       " '##aling': 3202,\n",
       " 'egypt': 6112,\n",
       " '##ho': 5797,\n",
       " 'simplistic': 7024,\n",
       " '##en': 141,\n",
       " 'veter': 4313,\n",
       " 'godfather': 5424,\n",
       " 'tod': 7931,\n",
       " 'cros': 15184,\n",
       " 'wayans': 12295,\n",
       " '##using': 2052,\n",
       " 'shades': 16787,\n",
       " 'mode': 8806,\n",
       " '##rett': 7521,\n",
       " '##hop': 14219,\n",
       " 'blister': 15175,\n",
       " 'bert': 11356,\n",
       " 'mra': 9806,\n",
       " '##xi': 9952,\n",
       " 'cube': 7637,\n",
       " 'bloch': 10316,\n",
       " 'surreal': 4368,\n",
       " 'baseketball': 7779,\n",
       " 'dion': 13896,\n",
       " 'notre': 14672,\n",
       " '##q': 117,\n",
       " 'europeans': 16528,\n",
       " 'grab': 10649,\n",
       " 'astral': 17113,\n",
       " 'valientes': 12688,\n",
       " '##pins': 14345,\n",
       " 'existed': 16832,\n",
       " 'potentially': 5324,\n",
       " '007': 10922,\n",
       " 'superlative': 17966,\n",
       " '##adel': 5832,\n",
       " 'undeni': 6322,\n",
       " 'invasion': 10826,\n",
       " 'fallen': 6909,\n",
       " 'tricks': 17240,\n",
       " 'goremeister': 18299,\n",
       " '##zum': 3902,\n",
       " 'ruins': 15700,\n",
       " 'impression': 3718,\n",
       " 'newly': 8112,\n",
       " 'stifler': 17891,\n",
       " 'burn': 3873,\n",
       " 'laughs': 3826,\n",
       " '##por': 3901,\n",
       " 'safe': 4956,\n",
       " 'wilder': 6462,\n",
       " 'recyc': 9009,\n",
       " 'amore': 14858,\n",
       " 'tenant': 9193,\n",
       " 'someth': 6823,\n",
       " 'variable': 16207,\n",
       " '##olo': 10131,\n",
       " '##klar': 14311,\n",
       " 'hilario': 12579,\n",
       " 'grieving': 15236,\n",
       " 'fasc': 1813,\n",
       " '##mate': 14358,\n",
       " '##omatic': 14538,\n",
       " '##force': 5011,\n",
       " 'normally': 1649,\n",
       " 'overl': 3061,\n",
       " 'explosive': 17492,\n",
       " 'colo': 15531,\n",
       " 'melville': 6412,\n",
       " 'herself': 9075,\n",
       " '##cle': 2451,\n",
       " '##elope': 10125,\n",
       " 'tomas': 12600,\n",
       " 'casted': 15538,\n",
       " 'assault': 6906,\n",
       " 'minnie': 10380,\n",
       " 'territ': 10344,\n",
       " '##vello': 14550,\n",
       " 'budd': 7444,\n",
       " '##kan': 7886,\n",
       " '##stre': 5219,\n",
       " 'hype': 3169,\n",
       " 'exit': 11886,\n",
       " 'norm': 1430,\n",
       " 'stopped': 5650,\n",
       " '##queen': 7517,\n",
       " 'begrud': 18158,\n",
       " '##ca': 2265,\n",
       " 'acad': 3311,\n",
       " 'fass': 7128,\n",
       " 'busby': 12881,\n",
       " '##ault': 6202,\n",
       " '##lesinger': 12333,\n",
       " '##vir': 3283,\n",
       " 'tec': 12102,\n",
       " 'never': 476,\n",
       " 'hy': 1516,\n",
       " 'grabbing': 17359,\n",
       " 'gun': 2832,\n",
       " 'everybody': 2864,\n",
       " 'kari': 12816,\n",
       " 'lost': 1493,\n",
       " 'doodleb': 13578,\n",
       " 'ppv': 5730,\n",
       " 'henson': 7136,\n",
       " '##ature': 5486,\n",
       " 'unspe': 8934,\n",
       " 'ape': 7109,\n",
       " 'filmfour': 14544,\n",
       " 'typing': 10559,\n",
       " '##iding': 7261,\n",
       " 'ghibli': 10704,\n",
       " 'unlucky': 11239,\n",
       " 'undis': 9015,\n",
       " 'locat': 12591,\n",
       " '##selves': 5097,\n",
       " 'ray': 2487,\n",
       " 'flopped': 13358,\n",
       " 'hollywood': 1114,\n",
       " 'chase': 4173,\n",
       " '##hat': 5189,\n",
       " 'softcore': 8377,\n",
       " 'romanti': 9221,\n",
       " 'usher': 12163,\n",
       " '##ude': 2797,\n",
       " 'ma': 319,\n",
       " 'tyr': 12366,\n",
       " 'sprawling': 8614,\n",
       " 'cringe': 7705,\n",
       " 'ze': 4156,\n",
       " '##cil': 4671,\n",
       " 'quent': 5545,\n",
       " 'agust': 12082,\n",
       " 'arrive': 16766,\n",
       " 'zodiac': 11295,\n",
       " 'unsentiment': 16839,\n",
       " 'wher': 10000,\n",
       " 'greet': 3926,\n",
       " '0': 20,\n",
       " '##que': 1307,\n",
       " 'vi': 1518,\n",
       " 'artistically': 17166,\n",
       " 'dah': 5752,\n",
       " '##gol': 14226,\n",
       " '##ions': 1826,\n",
       " 'numbers': 3261,\n",
       " 'freeman': 6402,\n",
       " 'trilog': 3641,\n",
       " 'hatch': 11418,\n",
       " 'stuck': 5603,\n",
       " 'vol': 6187,\n",
       " 'radiant': 16421,\n",
       " '##umble': 5260,\n",
       " '##away': 5938,\n",
       " 'similarly': 16705,\n",
       " 'jon': 1917,\n",
       " '##cat': 14209,\n",
       " 'dia': 6753,\n",
       " 'comprised': 14733,\n",
       " 'kook': 11433,\n",
       " 'intellect': 4860,\n",
       " 'cin': 775,\n",
       " 'purch': 2509,\n",
       " 'direction': 2664,\n",
       " '##ented': 7251,\n",
       " 'collabor': 6989,\n",
       " 'consid': 8151,\n",
       " 'none': 3193,\n",
       " '##lyss': 11731,\n",
       " 'rgv': 18116,\n",
       " 'editing': 3952,\n",
       " 'cath': 6152,\n",
       " '##bel': 1898,\n",
       " '##it': 161,\n",
       " 'aust': 1266,\n",
       " 'stark': 8994,\n",
       " 'selling': 6244,\n",
       " 'cassandra': 10813,\n",
       " 'knack': 10207,\n",
       " 'flashback': 10868,\n",
       " 'promising': 3510,\n",
       " 'sitco': 15888,\n",
       " 'sheer': 4080,\n",
       " 'unsettling': 11210,\n",
       " 'engross': 6864,\n",
       " 'overwhelming': 9604,\n",
       " 'governess': 17750,\n",
       " '##jar': 7195,\n",
       " 'possi': 10458,\n",
       " '##ink': 356,\n",
       " '##yle': 14317,\n",
       " 'cb4': 18057,\n",
       " 'ingr': 5824,\n",
       " 'dissapo': 17342,\n",
       " '##vagan': 14336,\n",
       " '##ush': 1801,\n",
       " 'ficti': 13435,\n",
       " 'nymph': 13036,\n",
       " 'nose': 14980,\n",
       " 'gilmore': 7679,\n",
       " 'disagreed': 16500,\n",
       " '##ette': 3788,\n",
       " 'lee': 2096,\n",
       " 'mastorak': 17394,\n",
       " 'espionage': 13227,\n",
       " 'terminus': 17588,\n",
       " 'steph': 1951,\n",
       " 'origin': 575,\n",
       " 'po': 340,\n",
       " 'video': 848,\n",
       " 'rar': 2085,\n",
       " 'discount': 7622,\n",
       " 'pitch': 3660,\n",
       " 'rumours': 13720,\n",
       " 'fiance': 13939,\n",
       " 'shue': 14680,\n",
       " 'pert': 15206,\n",
       " '##hay': 11572,\n",
       " 'radiofre': 16710,\n",
       " '##atement': 14422,\n",
       " 'nuovomondo': 10924,\n",
       " 'shelley': 13170,\n",
       " 'vanessa': 9282,\n",
       " 'cole': 7393,\n",
       " 'restraint': 12931,\n",
       " 'abus': 11858,\n",
       " 'behi': 16328,\n",
       " 'apocalypse': 7684,\n",
       " 'triad': 9215,\n",
       " 'oeu': 14064,\n",
       " 'regret': 4582,\n",
       " 'vancouver': 8595,\n",
       " 'oli': 2359,\n",
       " 'acknowledging': 17688,\n",
       " 'submitted': 16976,\n",
       " 'tran': 4515,\n",
       " 'thing': 494,\n",
       " '##gingly': 14230,\n",
       " 'vlad': 17718,\n",
       " '##aso': 5488,\n",
       " 'mediev': 16253,\n",
       " 'knef': 14995,\n",
       " 'carlo': 10761,\n",
       " 'walmart': 9168,\n",
       " 'steady': 12280,\n",
       " 'shower': 8927,\n",
       " 'spheeris': 11307,\n",
       " 'swear': 5308,\n",
       " 'rooney': 12134,\n",
       " 'lol': 5516,\n",
       " 'medical': 6009,\n",
       " '##ooo': 10046,\n",
       " 'marri': 6826,\n",
       " '##lying': 9987,\n",
       " 'travell': 17642,\n",
       " 'doo': 4300,\n",
       " 'boxing': 10687,\n",
       " 'shep': 6724,\n",
       " 'geniu': 15404,\n",
       " '##aya': 14621,\n",
       " 'haunting': 4113,\n",
       " 'spoiled': 8066,\n",
       " 'meaning': 3502,\n",
       " '##ica': 1464,\n",
       " 'ramp': 14097,\n",
       " 'isla': 11705,\n",
       " 'features': 2149,\n",
       " 'repe': 3575,\n",
       " 'goebbels': 17549,\n",
       " 'underdevel': 15361,\n",
       " 'specia': 15534,\n",
       " 'divorce': 8568,\n",
       " 'deter': 9278,\n",
       " 'enjoya': 15214,\n",
       " 'moviego': 11718,\n",
       " 'animat': 5601,\n",
       " 'unsuspect': 16840,\n",
       " 'springer': 7686,\n",
       " '##bluth': 17043,\n",
       " 'unmiss': 13292,\n",
       " 'americana': 15653,\n",
       " '##edge': 14465,\n",
       " 'praised': 6964,\n",
       " 'viciously': 13569,\n",
       " '##eretta': 13596,\n",
       " 'roc': 15146,\n",
       " 'dreck': 5959,\n",
       " 'texas': 3027,\n",
       " 'contains': 1472,\n",
       " '##izer': 8123,\n",
       " 'spiritually': 16732,\n",
       " 'pistol': 18109,\n",
       " 'greta': 7306,\n",
       " 'incon': 10456,\n",
       " 'themes': 5984,\n",
       " 'enigmatic': 17740,\n",
       " 'faithful': 5394,\n",
       " 'beloved': 6552,\n",
       " '##end': 456,\n",
       " 'epit': 5912,\n",
       " 'opinionated': 15979,\n",
       " '##sp': 940,\n",
       " 'meteorite': 17303,\n",
       " 'attract': 4262,\n",
       " '##prom': 16599,\n",
       " '##orde': 9968,\n",
       " '2008': 3704,\n",
       " 'shrewd': 13631,\n",
       " 'larkin': 18094,\n",
       " 'wom': 889,\n",
       " 'drummond': 17297,\n",
       " 'occasion': 13045,\n",
       " 'strings': 15430,\n",
       " 'signi': 16685,\n",
       " 'phone': 7457,\n",
       " '##majer': 16186,\n",
       " 'tint': 9845,\n",
       " 'replaces': 17726,\n",
       " 'struggle': 5404,\n",
       " 'cun': 7114,\n",
       " 'tut': 7175,\n",
       " 'tidy': 14133,\n",
       " 'unequivocally': 18283,\n",
       " 'contempor': 6532,\n",
       " 'carry': 4744,\n",
       " 'lina': 14027,\n",
       " '##oi': 11596,\n",
       " 'awarded': 12896,\n",
       " 'imagined': 10589,\n",
       " '##gera': 10679,\n",
       " 'deodato': 11309,\n",
       " 'nico': 16336,\n",
       " 'unease': 16622,\n",
       " 'opini': 12609,\n",
       " 'issues': 6074,\n",
       " 'crow': 4538,\n",
       " 'incons': 15546,\n",
       " 'painfully': 3974,\n",
       " 'romant': 1384,\n",
       " 'entrepreneur': 18255,\n",
       " 'dinosaur': 6086,\n",
       " 'cuba': 5417,\n",
       " 'observed': 8338,\n",
       " '##nnnnnnnn': 17585,\n",
       " 'z': 68,\n",
       " 'davies': 5339,\n",
       " 'gosling': 10237,\n",
       " 'recent': 1969,\n",
       " 'lacked': 6439,\n",
       " 'zer': 14180,\n",
       " 'internat': 10738,\n",
       " '##3': 125,\n",
       " 'everyone': 1120,\n",
       " 'goodnight': 6750,\n",
       " 'murder': 1849,\n",
       " 'xiz': 14171,\n",
       " 'cannibal': 6386,\n",
       " '##friars': 18146,\n",
       " '##gopal': 14233,\n",
       " '##01': 9950,\n",
       " 'begotten': 6524,\n",
       " 'cly': 9755,\n",
       " '##ipped': 5355,\n",
       " 'abso': 10387,\n",
       " 'situation': 4784,\n",
       " '##ice': 539,\n",
       " 'granny': 8088,\n",
       " 'careers': 15934,\n",
       " 'plea': 11079,\n",
       " 'sides': 11506,\n",
       " 'imitates': 13609,\n",
       " '##istry': 14950,\n",
       " '##ahoma': 15651,\n",
       " 'poo': 9820,\n",
       " 'carpent': 2956,\n",
       " 'deser': 1569,\n",
       " 'comedians': 7382,\n",
       " 'gracie': 16038,\n",
       " 'sk': 1594,\n",
       " 'critic': 1988,\n",
       " 'unger': 12046,\n",
       " '##olph': 2985,\n",
       " 'syndicated': 17171,\n",
       " 'describes': 5088,\n",
       " 'west': 2407,\n",
       " 'manch': 10160,\n",
       " 'godzilla': 9141,\n",
       " 'condi': 16790,\n",
       " 'prin': 3379,\n",
       " 'carnosaur': 9620,\n",
       " 'reun': 5220,\n",
       " 'aggressive': 17630,\n",
       " '##nton': 9172,\n",
       " 'anonymous': 14490,\n",
       " 'duckman': 17141,\n",
       " 'grud': 4541,\n",
       " 'turkish': 4292,\n",
       " 'jelinek': 18081,\n",
       " 'common': 3713,\n",
       " 'kiefer': 13756,\n",
       " 'toy': 6691,\n",
       " 'drunk': 9030,\n",
       " 'nothin': 7962,\n",
       " 'mat': 936,\n",
       " '##lest': 11768,\n",
       " 'favel': 15017,\n",
       " 'travelling': 17643,\n",
       " 'filmmakers': 3743,\n",
       " '##dain': 11612,\n",
       " '1974': 4767,\n",
       " 'kaige': 13759,\n",
       " 'movie': 154,\n",
       " 'liners': 10898,\n",
       " '##ss': 311,\n",
       " 'amazingly': 5611,\n",
       " 'directed': 945,\n",
       " 'jp': 11423,\n",
       " 'dassin': 11386,\n",
       " 'feet': 10214,\n",
       " 'psychological': 4820,\n",
       " '##la': 4488,\n",
       " '##uso': 10307,\n",
       " 'dahl': 11076,\n",
       " 'hist': 1084,\n",
       " 'avery': 16192,\n",
       " 'playing': 2243,\n",
       " 'trekkies': 16639,\n",
       " 'of': 146,\n",
       " 'respectable': 16652,\n",
       " '##mi': 5811,\n",
       " 'tues': 14129,\n",
       " 'transported': 17591,\n",
       " 'trem': 4311,\n",
       " 'peck': 4973,\n",
       " '##asure': 3457,\n",
       " 'emmanuelle': 15615,\n",
       " 'slowest': 12595,\n",
       " 'ichikawa': 13526,\n",
       " 'aspects': 4426,\n",
       " 'universally': 12979,\n",
       " 'sound': 1213,\n",
       " 'costello': 7580,\n",
       " '##ublic': 3093,\n",
       " 'rathbone': 7721,\n",
       " 'modine': 12820,\n",
       " 'tng': 17867,\n",
       " '##eday': 11710,\n",
       " 'homophobic': 13836,\n",
       " 'deaths': 16042,\n",
       " 'lass': 7822,\n",
       " 'greater': 10177,\n",
       " 'blood': 1540,\n",
       " 'division': 14921,\n",
       " 'familiar': 2419,\n",
       " 'alexandra': 8422,\n",
       " 'disc': 2755,\n",
       " '##zzi': 16105,\n",
       " 'funniest': 1485,\n",
       " 'elfriede': 17841,\n",
       " '##ase': 1221,\n",
       " 'mermaid': 8292,\n",
       " 'timberlake': 11312,\n",
       " 'pufnstuf': 17988,\n",
       " 'nadir': 17705,\n",
       " 'consegu': 14992,\n",
       " 'norwegian': 8158,\n",
       " '##bbe': 11660,\n",
       " 'rachael': 8436,\n",
       " 'ratings': 4776,\n",
       " '##pd': 14337,\n",
       " 'sketch': 8271,\n",
       " '##isms': 16108,\n",
       " 'else': 2010,\n",
       " 'drop': 8110,\n",
       " 'beca': 4531,\n",
       " 'grinders': 17761,\n",
       " 'lund': 6171,\n",
       " '##omy': 7942,\n",
       " 'vad': 11526,\n",
       " 'hunter': 3835,\n",
       " 'family': 1027,\n",
       " 'opp': 2447,\n",
       " 'tract': 15387,\n",
       " 'rerun': 9837,\n",
       " '##ved': 435,\n",
       " '##urg': 11864,\n",
       " 'court': 6435,\n",
       " 'rou': 5777,\n",
       " '##craft': 7685,\n",
       " 'plainf': 16282,\n",
       " 'vega': 12004,\n",
       " '##angers': 4208,\n",
       " 'made': 400,\n",
       " 'solution': 10705,\n",
       " 'througho': 12343,\n",
       " 'intending': 12022,\n",
       " 'bettany': 13808,\n",
       " 'workers': 7422,\n",
       " 'dominion': 10995,\n",
       " 'immediately': 4287,\n",
       " 'piec': 10803,\n",
       " '##terds': 11872,\n",
       " 'rush': 4908,\n",
       " 'fami': 5588,\n",
       " '##aking': 1479,\n",
       " 'aid': 9741,\n",
       " 'upsc': 15110,\n",
       " 'ernie': 15862,\n",
       " 'kelly': 4141,\n",
       " '##rier': 14769,\n",
       " '##icator': 14571,\n",
       " 'muni': 17080,\n",
       " 'tennant': 15893,\n",
       " 'unhing': 7342,\n",
       " 'fourth': 3830,\n",
       " 'wee': 7286,\n",
       " 'cooking': 11044,\n",
       " 'knowles': 12058,\n",
       " 'cem': 13886,\n",
       " 'lengthy': 10859,\n",
       " 'enf': 11950,\n",
       " '##tie': 14265,\n",
       " '1927': 7632,\n",
       " 'cummings': 11222,\n",
       " 'british': 1371,\n",
       " 'ef': 1662,\n",
       " 'fassbinder': 8574,\n",
       " '##leine': 11769,\n",
       " '##ett': 1780,\n",
       " 'sequel': 1146,\n",
       " 'ultim': 2730,\n",
       " 'whoever': 3298,\n",
       " 'funeral': 9643,\n",
       " 'paddy': 17321,\n",
       " 'ameri': 5289,\n",
       " 'begr': 14528,\n",
       " 'yul': 8733,\n",
       " 'idealized': 17299,\n",
       " 'remaking': 15133,\n",
       " 'ok': 444,\n",
       " '+': 15,\n",
       " 'establishme': 17583,\n",
       " '##venile': 10217,\n",
       " 'slipped': 10368,\n",
       " 'lem': 5161,\n",
       " 'emperor': 7462,\n",
       " 'intruder': 8305,\n",
       " 'keys': 10448,\n",
       " '##ored': 2493,\n",
       " 'efforts': 5078,\n",
       " 'sixtie': 16239,\n",
       " '##ray': 4484,\n",
       " 'adapted': 4392,\n",
       " 'dazz': 13912,\n",
       " '##gl': 2836,\n",
       " 'narr': 3754,\n",
       " '##leton': 7945,\n",
       " '##standing': 2544,\n",
       " 'look': 463,\n",
       " 'mentio': 13187,\n",
       " 'middle': 2100,\n",
       " 'flats': 15227,\n",
       " '##ute': 2304,\n",
       " 'miragl': 16823,\n",
       " 'diam': 5249,\n",
       " 'acce': 15329,\n",
       " '##plete': 12146,\n",
       " 'gardens': 8424,\n",
       " 'chilling': 6080,\n",
       " 'bulgar': 13224,\n",
       " 'entertaining': 1296,\n",
       " '##une': 3131,\n",
       " 'theod': 11676,\n",
       " '##ff': 457,\n",
       " 'playi': 15488,\n",
       " 'completing': 12422,\n",
       " 'at': 236,\n",
       " 'argentine': 10949,\n",
       " 'egy': 4889,\n",
       " '##ographer': 5416,\n",
       " '3rd': 5741,\n",
       " 'winter': 5976,\n",
       " 'shiel': 14686,\n",
       " 'mazz': 14959,\n",
       " 'mai': 8022,\n",
       " 'preaching': 17413,\n",
       " 'field': 6596,\n",
       " 'sub': 987,\n",
       " 'lon': 2394,\n",
       " 'troubles': 16773,\n",
       " 'hagen': 14630,\n",
       " 'spirited': 5666,\n",
       " 'haunt': 10846,\n",
       " 'awfu': 8075,\n",
       " 'o': 57,\n",
       " '##gan': 1679,\n",
       " '##cular': 11568,\n",
       " 'lowest': 7433,\n",
       " 'glob': 12447,\n",
       " 'lola': 11902,\n",
       " 'wrong': 1389,\n",
       " '##irmed': 11820,\n",
       " 'viva': 9480,\n",
       " 'doing': 3294,\n",
       " 'goofs': 9430,\n",
       " '##zona': 9939,\n",
       " 'overacts': 17558,\n",
       " 'opens': 2420,\n",
       " 'dramedy': 18233,\n",
       " 'albert': 3795,\n",
       " 'accid': 15634,\n",
       " '##otely': 8828,\n",
       " '##won': 9922,\n",
       " 'foc': 2866,\n",
       " 'exact': 1588,\n",
       " 'countless': 5649,\n",
       " 'dislike': 4056,\n",
       " 'qui': 5261,\n",
       " 'raccoon': 16682,\n",
       " '##all': 362,\n",
       " 'repeat': 5397,\n",
       " 'shaped': 17730,\n",
       " 'wh': 169,\n",
       " 'flag': 8086,\n",
       " '1995': 4395,\n",
       " '##oto': 14560,\n",
       " 'amnes': 11940,\n",
       " '##zov': 14352,\n",
       " 'inspi': 12832,\n",
       " 'sands': 10747,\n",
       " 'doubt': 1061,\n",
       " 'vow': 9855,\n",
       " '##ive': 342,\n",
       " '##ate': 331,\n",
       " 'caps': 13892,\n",
       " '##erson': 883,\n",
       " '##uts': 10035,\n",
       " '##ators': 5211,\n",
       " 'valid': 16072,\n",
       " 'capturing': 16349,\n",
       " 'influenced': 17109,\n",
       " 'against': 2348,\n",
       " 'boys': 3006,\n",
       " 'cri': 5440,\n",
       " 'chayevsky': 17614,\n",
       " '##shay': 15627,\n",
       " 'cameroon': 16357,\n",
       " 'changer': 15479,\n",
       " '##ney': 666,\n",
       " '##mare': 14362,\n",
       " 'mixt': 6851,\n",
       " '##lder': 5230,\n",
       " 'portra': 4789,\n",
       " 'cau': 3348,\n",
       " '##led': 944,\n",
       " '##tr': 11593,\n",
       " 'encounters': 17265,\n",
       " 'behind': 2104,\n",
       " 'horrorfest': 15247,\n",
       " 'dudley': 13183,\n",
       " 'spl': 2708,\n",
       " 'principle': 17263,\n",
       " 'clint': 3055,\n",
       " '16': 4295,\n",
       " 'extremely': 1320,\n",
       " 'been': 349,\n",
       " 'contribution': 8546,\n",
       " 'mamet': 7025,\n",
       " '##uan': 14300,\n",
       " 'vanilla': 10697,\n",
       " 'thi': 798,\n",
       " 'gamer': 9781,\n",
       " 'friends': 1180,\n",
       " 'cameras': 16356,\n",
       " '##enda': 8991,\n",
       " 'lecture': 13610,\n",
       " 'ont': 11747,\n",
       " 'calibre': 17496,\n",
       " 'path': 2162,\n",
       " 'demented': 8257,\n",
       " '##inee': 5265,\n",
       " 'maximum': 17816,\n",
       " 'disagree': 2555,\n",
       " 'sayer': 14998,\n",
       " 'invest': 6694,\n",
       " '##idy': 6719,\n",
       " 'agents': 8951,\n",
       " 'nominat': 13094,\n",
       " 'wilkinson': 18275,\n",
       " '##aste': 2989,\n",
       " 'tolerable': 17236,\n",
       " 'attem': 10428,\n",
       " 'adorable': 6105,\n",
       " 'deserted': 16126,\n",
       " 'tas': 6627,\n",
       " '##ules': 11615,\n",
       " '##azine': 5002,\n",
       " 'mani': 7312,\n",
       " 'misery': 15561,\n",
       " 'dads': 12969,\n",
       " '##ips': 3491,\n",
       " 'provide': 10787,\n",
       " 'pimlico': 17524,\n",
       " 'compared': 2770,\n",
       " '##ides': 2116,\n",
       " 'giorgino': 12387,\n",
       " 'will': 492,\n",
       " 'pleasure': 2281,\n",
       " 'awesom': 8505,\n",
       " 'rogers': 5554,\n",
       " 'prejud': 8583,\n",
       " '##zes': 7890,\n",
       " 'mcclure': 18004,\n",
       " '##ood': 246,\n",
       " 'mee': 14885,\n",
       " 'nashville': 17709,\n",
       " 'keen': 6351,\n",
       " 'lyn': 2647,\n",
       " 'shol': 10064,\n",
       " 'meet': 3471,\n",
       " 'togeth': 16399,\n",
       " 'morally': 15798,\n",
       " 'rigoletto': 18296,\n",
       " 'migh': 10523,\n",
       " 'simpsons': 9151,\n",
       " 'shoes': 16187,\n",
       " 'ic': 13976,\n",
       " '##nish': 14201,\n",
       " '##ume': 3225,\n",
       " 'cam': 4453,\n",
       " 'bir': 2890,\n",
       " 'showc': 5254,\n",
       " 'aali': 13865,\n",
       " 'deu': 14893,\n",
       " 'edgar': 5598,\n",
       " 'subsequ': 15697,\n",
       " 'turd': 12758,\n",
       " 'conspir': 6840,\n",
       " 'gran': 12180,\n",
       " '##ands': 14982,\n",
       " 'familia': 17060,\n",
       " 'succinct': 16113,\n",
       " '##oose': 7257,\n",
       " '##izu': 10412,\n",
       " 'dies': 4455,\n",
       " 'marries': 10434,\n",
       " 'monica': 15595,\n",
       " 'hahahah': 17892,\n",
       " 'timely': 15055,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32aa5a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer/vocab.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer.save_model(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cae28c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer.from_file(\"tokenizer/vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7472215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'oh', 'it', 'works', 'just', 'fine', '[SEP]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence = \\\n",
    "tokenizer.encode(\"Oh it works just fine\")\n",
    "tokenized_sentence.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8adb93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'oh',\n",
       " '##o',\n",
       " '##h',\n",
       " 'i',\n",
       " 'thoug',\n",
       " '##t',\n",
       " 'it',\n",
       " 'might',\n",
       " 'be',\n",
       " 'working',\n",
       " '##g',\n",
       " 'well',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence = \\\n",
    "tokenizer.encode(\"ohoh i thougt it might be workingg well\")\n",
    "tokenized_sentence.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "334ea9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file tokenizer/tokenizer.json. We won't load it.\n",
      "Didn't find file tokenizer/added_tokens.json. We won't load it.\n",
      "Didn't find file tokenizer/special_tokens_map.json. We won't load it.\n",
      "Didn't find file tokenizer/tokenizer_config.json. We won't load it.\n",
      "loading file tokenizer/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20186b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import LineByLineTextDataset\n",
    "dataset = \\\n",
    "LineByLineTextDataset(tokenizer=tokenizer,\n",
    "                      file_path=\"corpus.txt\", \n",
    "                      block_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "704d0c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "                      tokenizer=tokenizer, \n",
    "                      mlm=True, \n",
    "                      mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60d95fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "                      output_dir=\"BERT\",\n",
    "                      overwrite_output_dir=True,\n",
    "                      num_train_epochs=1,\n",
    "                      per_device_train_batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc962feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM\n",
    "bert = BertForMaskedLM(BertConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daa8194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(model=bert, \n",
    "                      args=training_args,\n",
    "                      data_collator=data_collator,\n",
    "                      train_dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "712085c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 50022\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 391\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='391' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [391/391 04:21, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=391, training_loss=5.366514396179667, metrics={'train_runtime': 263.1469, 'train_samples_per_second': 190.092, 'train_steps_per_second': 1.486, 'total_flos': 635727015135000.0, 'train_loss': 5.366514396179667, 'epoch': 1.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "253a56e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to MyBERT\n",
      "Configuration saved in MyBERT/config.json\n",
      "Model weights saved in MyBERT/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"MyBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aac1f55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e753c5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-11 17:30:50.048247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-11 17:30:50.464076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-11 17:30:50.464941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-11 17:30:50.466286: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-11 17:30:50.466621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-11 17:30:50.467282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-11 17:30:50.467958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-11 17:30:55.862709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-11 17:30:55.863401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-11 17:30:55.864025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-11 17:30:55.864586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12627 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from transformers import\\\n",
    "TFBertModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259cda35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer2 = BertTokenizerFast.from_pretrained(\n",
    "\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923c4bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.models.bert.modeling_tf_bert.TFBertMainLayer at 0x7fd6882d6730>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26cef144",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer2.batch_encode_plus(\n",
    "                   [\"hello how is it going with you\",\n",
    "                   \"lets test it\"], \n",
    "                    return_tensors=\"tf\", \n",
    "                    max_length=128, \n",
    "                    truncation=True, \n",
    "                    pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a5cebe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor: shape=(2, 128, 768), dtype=float32, numpy=\n",
       "array([[[ 0.10047169,  0.0677025 , -0.08335952, ..., -0.4933047 ,\n",
       "          0.11653915,  0.22664754],\n",
       "        [ 0.3236246 ,  0.37071785,  0.61468565, ..., -0.62726766,\n",
       "          0.37908283,  0.07053157],\n",
       "        [ 0.19953474, -0.87550944, -0.06478716, ..., -0.0128083 ,\n",
       "          0.30765152, -0.02073251],\n",
       "        ...,\n",
       "        [-0.20290422,  0.15799463,  0.60413766, ..., -0.17316641,\n",
       "         -0.02515502,  0.19266947],\n",
       "        [-0.1967066 ,  0.03543479,  0.65431905, ..., -0.07587673,\n",
       "         -0.06067057,  0.03544359],\n",
       "        [-0.24859604,  0.0245423 ,  0.6876988 , ..., -0.08229918,\n",
       "         -0.05705748,  0.05443959]],\n",
       "\n",
       "       [[ 0.02945649,  0.2308684 ,  0.292652  , ..., -0.13042173,\n",
       "          0.18965931,  0.46842816],\n",
       "        [ 1.7052323 ,  0.69135946,  0.7315114 , ...,  0.2893024 ,\n",
       "          0.53675807, -0.15455289],\n",
       "        [ 0.10459755,  0.09636812,  0.06996638, ..., -0.41592342,\n",
       "         -0.11898948, -0.6722405 ],\n",
       "        ...,\n",
       "        [ 0.52687496,  0.17353363,  0.62832326, ...,  0.04882293,\n",
       "          0.18276252,  0.07595042],\n",
       "        [ 0.43262482,  0.10590136,  0.600885  , ...,  0.08856198,\n",
       "          0.14158893, -0.10057041],\n",
       "        [-0.01098413, -0.2716704 ,  0.5400942 , ...,  0.39418104,\n",
       "          0.03240824, -0.20017639]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
       "array([[-0.9204854 , -0.37138966, -0.6051255 , ..., -0.44736958,\n",
       "        -0.6434757 ,  0.9423271 ],\n",
       "       [-0.88541585, -0.26547676,  0.2101491 , ...,  0.17237073,\n",
       "        -0.64029914,  0.88883436]], dtype=float32)>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15c6806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "max_length = 128\n",
    "tokens = keras.layers.Input(shape=(max_length,),\n",
    "                           dtype=tf.dtypes.int32)\n",
    "masks = keras.layers.Input(shape=(max_length,),\n",
    "                          dtype=tf.dtypes.int32)\n",
    "embedding_layer = bert.layers[0]([tokens,masks])[0][:,0,:]\n",
    "dense = tf.keras.layers.Dense(units=2, \n",
    "        activation=\"softmax\")(embedding_layer)\n",
    "model = keras.Model([tokens,masks],dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a54472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer2.batch_encode_plus(\n",
    "[\"hello how is it going with you\",\n",
    "\"hello how is it going with you\"], \n",
    "return_tensors=\"tf\", \n",
    "max_length= max_length, \n",
    "truncation=True, \n",
    "pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "928781a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[0.49622813, 0.50377184],\n",
       "       [0.49622813, 0.50377184]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model([tokenized[\"input_ids\"],tokenized[\"attention_mask\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8903ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (TFBertMainLayer)          multiple             109482240   input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_2 (Sli (None, 768)          0           bert[2][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            1538        tf.__operators__.getitem_2[0][0] \n",
      "==================================================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 1,538\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"Adam\",\n",
    "loss=\"categorical_crossentropy\", \n",
    "metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0f74f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b75e7c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (TFBertMainLayer)          multiple             109482240   input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_2 (Sli (None, 768)          0           bert[2][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            1538        tf.__operators__.getitem_2[0][0] \n",
      "==================================================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 1,538\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af099f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "imdb_df = pd.read_csv(\"IMDB-Dataset.csv\")\n",
    "reviews = list(imdb_df.review)\n",
    "tokenized_reviews = \\\n",
    "tokenizer2.batch_encode_plus(reviews, return_tensors=\"tf\",\n",
    "                           max_length= max_length,\n",
    "                           truncation=True, \n",
    "                           pad_to_max_length=True)\n",
    "import numpy as np\n",
    "train_split = int(0.8 * len(tokenized_reviews[\"attention_mask\"]))\n",
    "train_tokens = tokenized_reviews[\"input_ids\"][:train_split]\n",
    "test_tokens = tokenized_reviews[\"input_ids\"][train_split:]\n",
    "train_masks = tokenized_reviews[\"attention_mask\"][:train_split]\n",
    "test_masks = tokenized_reviews[\"attention_mask\"][train_split:]\n",
    "sentiments = list(imdb_df.sentiment)\n",
    "labels = np.array([[0,1] if sentiment == \"positive\" else [1,0] for sentiment in sentiments])\n",
    "train_labels = labels[:train_split]\n",
    "test_labels = labels[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8eb8f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1250/1250 [==============================] - 387s 304ms/step - loss: 0.4880 - accuracy: 0.7669\n",
      "Epoch 2/5\n",
      "1250/1250 [==============================] - 387s 309ms/step - loss: 0.4566 - accuracy: 0.7862\n",
      "Epoch 3/5\n",
      "1250/1250 [==============================] - 385s 308ms/step - loss: 0.4497 - accuracy: 0.7902\n",
      "Epoch 4/5\n",
      "1250/1250 [==============================] - 386s 309ms/step - loss: 0.4503 - accuracy: 0.7903\n",
      "Epoch 5/5\n",
      "1250/1250 [==============================] - 386s 309ms/step - loss: 0.4504 - accuracy: 0.7921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd206b45070>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([train_tokens,train_masks],train_labels, \n",
    "             epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fbf4e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7fd1dba53340>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
